{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import model\n",
        "import resnet\n",
        "import params\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from data import Dataset, Subset\n",
        "\n",
        "\n",
        "\n",
        "original_trainset = torchvision.datasets.CIFAR100(root= './data', train= True, transform= params.transform_train, download = True)\n",
        "\n",
        "original_testset = torchvision.datasets.CIFAR100(root = './data', train = False, transform= params.transform_test, download = True)\n",
        "\n",
        "# use our custom class for dataset\n",
        "train_Dataset = Dataset(original_trainset, classes_per_task= params.TASK_CLASSES,transform= params.transform_train)\n",
        "test_Dataset = Dataset(original_testset, classes_per_task = params.TASK_CLASSES,transform= params.transform_test)\n",
        "\n",
        "\n",
        "\n",
        "#check if the splits in train and test are equal\n",
        "assert (np.array_equal(train_Dataset.splits, test_Dataset.splits)), \"The splits are different, check the code!\"\n",
        "\n",
        "\n",
        "# number of splits = number of tasks\n",
        "splits =(train_Dataset.splits)\n",
        "n_tasks = splits.shape[0]\n",
        "splits = splits.tolist()\n",
        "print(\"Successful split. Number of tasks: \", n_tasks)\n",
        "\n",
        "train_indexes = []\n",
        "test_indexes = []\n",
        "\n",
        "\n",
        "ResNet=resnet.resnet32(num_classes=params.TASK_CLASSES)\n",
        "ResNet.to(params.DEVICE)\n",
        "\n",
        "exemplar_indexes = []\n",
        "exemplar_images  = []\n",
        "random.seed(params.SEED)\n",
        "np.random.seed(params.SEED)\n",
        "torch.manual_seed(params.SEED)\n",
        "for task in range(n_tasks):\n",
        "  known_classes= task*params.TASK_CLASSES\n",
        "  #indexes for this task\n",
        "  train_indexes =  train_Dataset.__getIndexesGroups__(task*params.TASK_CLASSES) # splits[task]\n",
        "  test_indexes = test_indexes + test_Dataset.__getIndexesGroups__(task*params.TASK_CLASSES)\n",
        "\n",
        "\n",
        "  ResNet ,train_set, train_loader = model.UpdateRepresentation(ResNet, task, train_Dataset, train_indexes, exemplar_indexes) #add neurons to fc and train\n",
        "\n",
        "\n",
        "  m = int(params.K/(known_classes+params.TASK_CLASSES)) #new number of exemplars for each class\n",
        "  m = int(m + .5)\n",
        "\n",
        "\n",
        "\n",
        "  for y,Py in enumerate(exemplar_indexes):\n",
        "    #print(len(exemplar_indexes))\n",
        "    exemplar_indexes[y]=model.ReduceExemplarSet(Py,m) #i take only the m most representative exemplars for each class\n",
        "\n",
        "  exemplar_indexes = model.ConstructExemplarSet(train_Dataset, train_indexes, m, ResNet, known_classes, exemplar_indexes) # take the exemplars for the new class\n",
        "\n",
        "\n",
        "  exemplar_means = model.ComputeMeansofExemplars(exemplar_indexes, ResNet,train_Dataset) #mean of the exemplars for each class\n",
        "    #end of task results\n",
        "  ResNet.eval()\n",
        "\n",
        "#results on training set\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  n_images = 0\n",
        "  correct_preds=0\n",
        "  for images, labels, _ in train_loader:\n",
        "    images = images.float().to(params.DEVICE) ## need to be float\n",
        "    labels = labels.to(params.DEVICE)\n",
        "    features = ResNet(images,features = True)\n",
        "    preds = model.classify(features, exemplar_means) #classify implements NME\n",
        "    n_images += len(images)\n",
        "    all_preds = np.concatenate((all_preds,preds))\n",
        "    all_labels = np.concatenate((all_labels,labels.cpu()))\n",
        "  correct_preds += (all_preds == all_labels).sum()\n",
        "  accuracy = correct_preds/n_images\n",
        "  print(f\"accuracy on training set: {accuracy}\")\n",
        "\n",
        "  #results on test set\n",
        "  test_set = Subset(test_Dataset,test_indexes,transform=params.transform_test)\n",
        "  test_loader = DataLoader( test_set, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  n_images = 0\n",
        "  correct_preds=0\n",
        "  for images, labels, _ in test_loader:\n",
        "    images = images.float().to(params.DEVICE) ## need to be float\n",
        "    labels = labels.to(params.DEVICE)\n",
        "    features = ResNet(images,features = True)\n",
        "    preds = model.classify(features, exemplar_means)\n",
        "    n_images += len(images)\n",
        "    all_preds = np.concatenate((all_preds,preds))\n",
        "    all_labels = np.concatenate((all_labels,labels.cpu()))\n",
        "  correct_preds += (all_preds == all_labels).sum()\n",
        "  accuracy = correct_preds/n_images\n",
        "  print(f\"accuracy on test set: {accuracy}\")\n",
        "\n",
        "\n",
        "  #confusion matrix\n",
        "  cm = confusion_matrix(all_labels,all_preds)\n",
        "  df_cm = pd.DataFrame(cm, range((task+1)*params.TASK_CLASSES), range((task+1)*params.TASK_CLASSES))\n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.heatmap(df_cm, annot=False, cmap=\"viridis\")\n",
        "  plt.savefig(f\"{task}_cf\")\n",
        "  ResNet = ResNet.train(True)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}